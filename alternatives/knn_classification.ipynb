{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roles Classifier Alternative: K-Nearest Neighbor\n",
    "\n",
    "Imports and downloading tokenizers from NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package genesis to /home/alexis/nltk_data...\n",
      "[nltk_data]   Package genesis is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/alexis/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/alexis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/alexis/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import genesis\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('genesis')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "genesis_ic = wn.ic(genesis, False, 0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-NN Classifier Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class KnnClassifier():\n",
    "    def __init__(self, k=1, distance_type='path'):\n",
    "        self.k = k\n",
    "        self.distance_type = distance_type\n",
    "\n",
    "    # This function is used for training\n",
    "    def fit(self, x_train, y_train):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    # This function runs the K(1) nearest neighbour algorithm and\n",
    "    # returns the label with closest match.\n",
    "    def predict(self, x_test):\n",
    "        self.x_test = x_test\n",
    "        y_predict = []\n",
    "\n",
    "        for i in range(len(x_test)):\n",
    "            max_sim = 0\n",
    "            max_index = 0\n",
    "            for j in range(self.x_train.shape[0]):\n",
    "                temp = self.document_similarity(x_test[i], self.x_train[j])\n",
    "                if temp > max_sim:\n",
    "                    max_sim = temp\n",
    "                    max_index = j\n",
    "            y_predict.append(self.y_train[max_index])\n",
    "        return y_predict\n",
    "\n",
    "    def convert_tag(self, tag):\n",
    "        \"\"\"Convert the tag given by nltk.pos_tag to the tag used by wordnet.synsets\"\"\"\n",
    "        tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}\n",
    "        try:\n",
    "            return tag_dict[tag[0]]\n",
    "        except KeyError:\n",
    "            return None\n",
    "\n",
    "    def doc_to_synsets(self, doc):\n",
    "        \"\"\"\n",
    "            Returns a list of synsets in document.\n",
    "            Tokenizes and tags the words in the document doc.\n",
    "            Then finds the first synset for each word/tag combination.\n",
    "            If a synset is not found for that combination it is skipped.\n",
    "\n",
    "            Args:\n",
    "                doc: string to be converted\n",
    "\n",
    "            Returns:\n",
    "                list of synsets\n",
    "        \"\"\"\n",
    "        tokens = word_tokenize(doc + ' ')\n",
    "\n",
    "        l = []\n",
    "        tags = nltk.pos_tag([tokens[0] + ' ']) if len(tokens) == 1 else nltk.pos_tag(tokens)\n",
    "\n",
    "        for token, tag in zip(tokens, tags):\n",
    "            syntag = self.convert_tag(tag[1])\n",
    "            syns = wn.synsets(token, syntag)\n",
    "            if (len(syns) > 0):\n",
    "                l.append(syns[0])\n",
    "        return l\n",
    "\n",
    "    def similarity_score(self, s1, s2, distance_type='path'):\n",
    "        \"\"\"\n",
    "        Calculate the normalized similarity score of s1 onto s2\n",
    "        For each synset in s1, finds the synset in s2 with the largest similarity value.\n",
    "        Sum of all of the largest similarity values and normalize this value by dividing it by the\n",
    "        number of largest similarity values found.\n",
    "\n",
    "        Args:\n",
    "            s1, s2: list of synsets from doc_to_synsets\n",
    "\n",
    "        Returns:\n",
    "            normalized similarity score of s1 onto s2\n",
    "        \"\"\"\n",
    "        s1_largest_scores = []\n",
    "\n",
    "        for i, s1_synset in enumerate(s1, 0):\n",
    "            max_score = 0\n",
    "            for s2_synset in s2:\n",
    "                if distance_type == 'path':\n",
    "                    score = s1_synset.path_similarity(s2_synset, simulate_root=False)\n",
    "                else:\n",
    "                    score = s1_synset.wup_similarity(s2_synset)\n",
    "                if score != None:\n",
    "                    if score > max_score:\n",
    "                        max_score = score\n",
    "\n",
    "            if max_score != 0:\n",
    "                s1_largest_scores.append(max_score)\n",
    "\n",
    "        mean_score = np.mean(s1_largest_scores)\n",
    "\n",
    "        return mean_score\n",
    "\n",
    "    def document_similarity(self, doc1, doc2):\n",
    "        \"\"\"Finds the symmetrical similarity between doc1 and doc2\"\"\"\n",
    "\n",
    "        synsets1 = self.doc_to_synsets(doc1)\n",
    "        synsets2 = self.doc_to_synsets(doc2)\n",
    "\n",
    "        return (self.similarity_score(synsets1, synsets2) + self.similarity_score(synsets2, synsets1)) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Several Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for file_size 150: 0.3\n",
      "Accuracy for file_size 200: 0.35\n",
      "Accuracy for file_size 250: 0.34\n",
      "Accuracy for file_size 300: 0.4166666666666667\n",
      "Accuracy for file_size 350: 0.5142857142857142\n",
      "Accuracy for file_size 400: 0.475\n",
      "Accuracy for file_size 450: 0.4222222222222222\n",
      "Accuracy for file_size 500: 0.45\n",
      "Accuracy for file_size 550: 0.37272727272727274\n",
      "Accuracy for file_size 600: 0.325\n"
     ]
    }
   ],
   "source": [
    "file_size = [150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 645]\n",
    "accuracy = []\n",
    "\n",
    "s = stopwords.words('english')\n",
    "ps = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "for i in file_size:\n",
    "    file_name = f'output/balanced_{i}.csv'\n",
    "    roles = pd.read_csv(f'../{file_name}')\n",
    "    mapping = {'Student': 0, 'Co-Facilitator': 1, 'Facilitator': 2}\n",
    "    roles['Role'] = roles['Role'].apply(lambda x: mapping[x])\n",
    "\n",
    "    for k in range(roles.shape[0]):\n",
    "        review = roles.loc[k, 'Text']\n",
    "        review = review.split()\n",
    "        review = [ps.lemmatize(word) for word in review if not word in s]\n",
    "        review = ' '.join(review)\n",
    "        roles.loc[k, 'Text'] = review\n",
    "\n",
    "    X = roles['Text']\n",
    "    y = roles['Role']\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # Train the Classifier\n",
    "    classifier = KnnClassifier(k=1, distance_type='path')\n",
    "    classifier.fit(X_train.values, y_train.values)\n",
    "\n",
    "    test_corpus = []\n",
    "    for x_value in X_valid.values:\n",
    "        review = x_value.split()\n",
    "\n",
    "        review = [ps.lemmatize(word) for word in review if not word in s]\n",
    "        review = ' '.join(review)\n",
    "        test_corpus.append(review)\n",
    "\n",
    "\n",
    "    def accuracy_score(y_pred, y_true):\n",
    "        matched = 0\n",
    "\n",
    "        for j in range(0, len(y_pred)):\n",
    "            if y_pred[j] == y_true[j]:\n",
    "                matched = matched + 1\n",
    "\n",
    "        return float(matched) / float(len(y_true))\n",
    "\n",
    "\n",
    "    error_count = 0\n",
    "    predictions = classifier.predict(test_corpus)\n",
    "\n",
    "    accuracy_partial = accuracy_score(list(map(int, predictions)), y_valid.values.tolist())\n",
    "    accuracy.append(accuracy_partial)\n",
    "    print(f'Accuracy for file_size {i}: {accuracy_partial}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphical Performance Analysis\n",
    "\n",
    "In the following plots we can see the how the model behaves when it is trained with different amounts of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(file_size, accuracy)\n",
    "plt.title('# of Rows vs. Accuracy')\n",
    "plt.suptitle('K-Nearest Neighbor Roles Classifier')\n",
    "plt.xlabel('# of Rows')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "- The model doesn't show a good performance with the datasets, and we can see that the behavior is random, from which we\n",
    "can conclude that KNN was not able to learn anything from the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}