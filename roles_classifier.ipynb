{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from BiLstmClassifier import BiLstmFixedLength, BiLstmVariableLength, BiLstmGloveVector\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 0. Hyper Parameters Definition\n",
    "Word embeddings are always around 50 and 300 in length, longer embedding vectors don't add enough information and\n",
    "smaller ones don't represent the semantics well enough.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 100  # Small batches because the dataset is not bigger than 500 rows\n",
    "HIDDEN_LAYER_DIM = 60  # AOLME is not too complex language, it represents the language's features\n",
    "EMBEDDED_LAYER_DIM = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(372, 2)\n",
      "      Role                           Text\n",
      "0  Student          you like how its like\n",
      "1  Student  its like youre obsessed with \n",
      "2  Student                   and this one\n",
      "3  Student                             no\n",
      "4  Student           i dont like summary \n"
     ]
    }
   ],
   "source": [
    "roles = pd.read_csv('output/balanced_20201008235520.csv')\n",
    "print(roles.shape)\n",
    "print(roles.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Mapping 'Roles' labels to numbers for vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mapping = {'Student': 0, 'Co-Facilitator': 1, 'Facilitator': 2}\n",
    "roles['Role'] = roles['Role'].apply(lambda x: mapping[x])\n",
    "roles.head()\n",
    "\n",
    "\n",
    "# Load English words model package\n",
    "tok = spacy.load('en')\n",
    "\n",
    "\n",
    "def tokenize(text: str):\n",
    "    \"\"\"\n",
    "    This method tokenizes a sentence, considering the text is already lowered,\n",
    "    ASCII, and  punctuation has been removed\n",
    "    :param text: The sentence to be tokenized\n",
    "    :return: A list containing each word of the sentence\n",
    "    \"\"\"\n",
    "    return [token.text for token in tok.tokenizer(text)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Dataset cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words before: 662\n",
      "num_words after: 347\n",
      "   Role                           Text  \\\n",
      "0     0          you like how its like   \n",
      "1     0  its like youre obsessed with    \n",
      "2     0                   and this one   \n",
      "3     0                             no   \n",
      "4     0           i dont like summary    \n",
      "\n",
      "                                          Vectorized  \n",
      "0  [[2, 3, 4, 5, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
      "1  [[5, 3, 2, 1, 1, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
      "2  [[7, 8, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
      "3  [[10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
      "4  [[11, 12, 13, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-81d6e3e92ab5>:38: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  roles['Vectorized'] = roles['Text'].apply(lambda x: np.array(encode_sentence(x, vocab2index)))\n"
     ]
    }
   ],
   "source": [
    "# Count number of occurrences of each word\n",
    "counts = Counter()\n",
    "for index, row in roles.iterrows():\n",
    "    counts.update(tokenize(row['Text']))\n",
    "\n",
    "# Deletes words appearing only once\n",
    "print(\"num_words before:\", len(counts.keys()))\n",
    "for word in list(counts):\n",
    "    if counts[word] < 2:\n",
    "        del counts[word]\n",
    "print(\"num_words after:\", len(counts.keys()))\n",
    "\n",
    "# Creates vocabulary\n",
    "vocab2index = {\"\": 0, \"UNK\": 1}\n",
    "words = [\"\", \"UNK\"]\n",
    "for word in counts:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)\n",
    "\n",
    "\n",
    "def encode_sentence(text, vocabulary_map, n=70):\n",
    "    \"\"\"\n",
    "    Encodes the sentence into a numerical vector, based on the vocabulary map\n",
    "    :param text: The sentence\n",
    "    :param vocabulary_map: A map assigning a number to each word in the vocabulary\n",
    "    :param n: Required vector size\n",
    "    :return: Vectorized sentence and length\n",
    "    \"\"\"\n",
    "    tokenized = tokenize(text)\n",
    "    vectorized = np.zeros(n, dtype=int)\n",
    "    enc1 = np.array([vocabulary_map.get(w, vocabulary_map[\"UNK\"]) for w in tokenized])\n",
    "    length = min(n, len(enc1))\n",
    "    vectorized[:length] = enc1[:length]\n",
    "    return vectorized, length\n",
    "\n",
    "\n",
    "# Creates a new column into Dataset: each sentence expressed as a numeric vector\n",
    "roles['Vectorized'] = roles['Text'].apply(lambda x: np.array(encode_sentence(x, vocab2index)))\n",
    "print(roles.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Check if the dataset is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 124, 1: 124, 2: 124})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(roles['Role'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Split into training and validation partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = list(roles['Vectorized'])\n",
    "y = list(roles['Role'])\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "class RolesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Simple PyTorch Dataset wrapper defined by an array of vectorized sentences (X) and the role for each sentence (y)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_x, input_y):\n",
    "        self.X = input_x\n",
    "        self.y = input_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx], self.X[idx][1]\n",
    "\n",
    "\n",
    "training_ds = RolesDataset(X_train, y_train)\n",
    "validation_ds = RolesDataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(input_model, epochs=10, lr=0.001):\n",
    "    \"\"\"\n",
    "    Trains the input model\n",
    "    :param input_model: Input Model\n",
    "    :param epochs: The number of training epochs\n",
    "    :param lr: Learning Rate\n",
    "    \"\"\"\n",
    "    parameters = filter(lambda p: p.requires_grad, input_model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        input_model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "\n",
    "        # Iterates on Training DataLoader\n",
    "        for x, y, l in training_dl:\n",
    "            x = x.long()\n",
    "            y = y.long()\n",
    "            y_pred = input_model(x, l)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item() * y.shape[0]\n",
    "            total += y.shape[0]\n",
    "\n",
    "        val_loss, val_acc, val_rmse = get_metrics(input_model, validation_dl)\n",
    "\n",
    "        if i % 10 == 1:\n",
    "            print(f\"Epoch {i}: training loss %.3f, valid. loss %.3f, valid. accuracy %.3f, and valid. RMSE %.3f\" % (\n",
    "                sum_loss / total, val_loss, val_acc, val_rmse))\n",
    "\n",
    "    print(f\"FINAL: training loss %.3f, valid. loss %.3f, valid. accuracy %.3f, and valid. RMSE %.3f\" % (\n",
    "        sum_loss / total, val_loss, val_acc, val_rmse))\n",
    "\n",
    "\n",
    "def get_metrics(input_model, valid_dl):\n",
    "    \"\"\"\n",
    "    Obtains current validation metrics\n",
    "    :param input_model: Input Model\n",
    "    :param valid_dl: Validation PyTorch DataLoader\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    input_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    sum_rmse = 0.0\n",
    "\n",
    "    # PyTorch uses CrossEntropy function to implement Softmax on the same function\n",
    "    for x, y, l in valid_dl:\n",
    "        x = x.long()\n",
    "        y = y.long()\n",
    "        y_hat = input_model(x, l)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        pred = torch.max(y_hat, 1)[1]\n",
    "        correct += (pred == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item() * y.shape[0]\n",
    "        sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1))) * y.shape[0]\n",
    "    return sum_loss / total, correct / total, sum_rmse / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(words)\n",
    "training_dl = DataLoader(training_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dl = DataLoader(validation_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## BiLSTM - Fixed Length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BiLSTM - Fixed Length: 100 epochs, Learning Rate: 0.1\n",
      "=============================================================\n",
      "Epoch 1: training loss 1.319, valid. loss 1.081, valid. accuracy 0.387, and valid. RMSE 1.222\n",
      "Epoch 11: training loss 0.353, valid. loss 1.387, valid. accuracy 0.573, and valid. RMSE 0.931\n",
      "Epoch 21: training loss 0.231, valid. loss 1.984, valid. accuracy 0.533, and valid. RMSE 1.013\n",
      "Epoch 31: training loss 0.201, valid. loss 2.211, valid. accuracy 0.507, and valid. RMSE 1.007\n",
      "Epoch 41: training loss 0.603, valid. loss 2.523, valid. accuracy 0.493, and valid. RMSE 1.052\n",
      "Epoch 51: training loss 0.388, valid. loss 2.242, valid. accuracy 0.467, and valid. RMSE 1.026\n",
      "Epoch 61: training loss 0.272, valid. loss 2.490, valid. accuracy 0.467, and valid. RMSE 0.966\n",
      "Epoch 71: training loss 0.315, valid. loss 2.470, valid. accuracy 0.520, and valid. RMSE 0.980\n",
      "Epoch 81: training loss 0.289, valid. loss 2.418, valid. accuracy 0.547, and valid. RMSE 0.945\n",
      "Epoch 91: training loss 0.228, valid. loss 2.091, valid. accuracy 0.533, and valid. RMSE 0.931\n",
      "FINAL: training loss 0.257, valid. loss 2.275, valid. accuracy 0.547, and valid. RMSE 0.966\n",
      "\n",
      "BiLSTM - Fixed Length: 100 epochs, Learning Rate: 0.05\n",
      "=============================================================\n",
      "Epoch 1: training loss 0.238, valid. loss 2.505, valid. accuracy 0.520, and valid. RMSE 0.959\n",
      "Epoch 11: training loss 0.184, valid. loss 2.452, valid. accuracy 0.560, and valid. RMSE 0.938\n",
      "Epoch 21: training loss 0.206, valid. loss 2.576, valid. accuracy 0.533, and valid. RMSE 0.993\n",
      "Epoch 31: training loss 0.160, valid. loss 2.745, valid. accuracy 0.507, and valid. RMSE 0.966\n",
      "Epoch 41: training loss 0.140, valid. loss 2.814, valid. accuracy 0.493, and valid. RMSE 1.013\n",
      "Epoch 51: training loss 0.135, valid. loss 2.879, valid. accuracy 0.493, and valid. RMSE 0.973\n",
      "Epoch 61: training loss 0.152, valid. loss 2.790, valid. accuracy 0.533, and valid. RMSE 0.993\n",
      "Epoch 71: training loss 0.122, valid. loss 2.731, valid. accuracy 0.560, and valid. RMSE 0.938\n",
      "Epoch 81: training loss 0.117, valid. loss 2.679, valid. accuracy 0.560, and valid. RMSE 0.959\n",
      "Epoch 91: training loss 0.124, valid. loss 2.714, valid. accuracy 0.600, and valid. RMSE 0.872\n",
      "FINAL: training loss 0.158, valid. loss 2.925, valid. accuracy 0.560, and valid. RMSE 0.917\n",
      "\n",
      "BiLSTM - Fixed Length: 100 epochs, Learning Rate: 0.01\n",
      "=============================================================\n",
      "Epoch 1: training loss 0.146, valid. loss 2.929, valid. accuracy 0.533, and valid. RMSE 0.973\n",
      "Epoch 11: training loss 0.099, valid. loss 3.023, valid. accuracy 0.573, and valid. RMSE 0.952\n",
      "Epoch 21: training loss 0.105, valid. loss 3.056, valid. accuracy 0.600, and valid. RMSE 0.872\n",
      "Epoch 31: training loss 0.100, valid. loss 3.055, valid. accuracy 0.573, and valid. RMSE 0.931\n",
      "Epoch 41: training loss 0.098, valid. loss 3.055, valid. accuracy 0.573, and valid. RMSE 0.952\n",
      "Epoch 51: training loss 0.107, valid. loss 3.072, valid. accuracy 0.573, and valid. RMSE 0.952\n",
      "Epoch 61: training loss 0.095, valid. loss 3.089, valid. accuracy 0.587, and valid. RMSE 0.945\n",
      "Epoch 71: training loss 0.101, valid. loss 3.064, valid. accuracy 0.587, and valid. RMSE 0.924\n",
      "Epoch 81: training loss 0.085, valid. loss 3.025, valid. accuracy 0.587, and valid. RMSE 0.924\n",
      "Epoch 91: training loss 0.096, valid. loss 3.047, valid. accuracy 0.587, and valid. RMSE 0.924\n",
      "FINAL: training loss 0.102, valid. loss 3.032, valid. accuracy 0.600, and valid. RMSE 0.917\n"
     ]
    }
   ],
   "source": [
    "model_fixed = BiLstmFixedLength(vocab_size, EMBEDDED_LAYER_DIM, HIDDEN_LAYER_DIM)\n",
    "\n",
    "print(f'\\nBiLSTM - Fixed Length: {EPOCHS} epochs, Learning Rate: 0.1')\n",
    "print('=============================================================')\n",
    "train_model(model_fixed, epochs=EPOCHS, lr=0.1)\n",
    "print(f'\\nBiLSTM - Fixed Length: {EPOCHS} epochs, Learning Rate: 0.05')\n",
    "print('=============================================================')\n",
    "train_model(model_fixed, epochs=EPOCHS, lr=0.05)\n",
    "print(f'\\nBiLSTM - Fixed Length: {EPOCHS} epochs, Learning Rate: 0.01')\n",
    "print('=============================================================')\n",
    "train_model(model_fixed, epochs=EPOCHS, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## BiLSTM - Variable Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BiLSTM - Variable Length: 100 epochs, Learning Rate: 0.1\n",
      "=============================================================\n",
      "Epoch 1: training loss 1.100, valid. loss 1.238, valid. accuracy 0.347, and valid. RMSE 0.980\n",
      "Epoch 11: training loss 0.344, valid. loss 1.634, valid. accuracy 0.520, and valid. RMSE 1.020\n",
      "Epoch 21: training loss 0.275, valid. loss 1.796, valid. accuracy 0.453, and valid. RMSE 1.033\n",
      "Epoch 31: training loss 0.287, valid. loss 1.873, valid. accuracy 0.600, and valid. RMSE 0.872\n",
      "Epoch 41: training loss 0.342, valid. loss 2.175, valid. accuracy 0.613, and valid. RMSE 0.864\n",
      "Epoch 51: training loss 0.340, valid. loss 2.042, valid. accuracy 0.573, and valid. RMSE 0.993\n",
      "Epoch 61: training loss 0.377, valid. loss 2.014, valid. accuracy 0.547, and valid. RMSE 1.046\n",
      "Epoch 71: training loss 0.403, valid. loss 2.018, valid. accuracy 0.613, and valid. RMSE 0.952\n",
      "Epoch 81: training loss 0.336, valid. loss 2.092, valid. accuracy 0.507, and valid. RMSE 1.065\n",
      "Epoch 91: training loss 0.381, valid. loss 1.819, valid. accuracy 0.547, and valid. RMSE 1.007\n",
      "FINAL: training loss 0.413, valid. loss 1.857, valid. accuracy 0.493, and valid. RMSE 1.033\n",
      "\n",
      "BiLSTM - Variable Length: 100 epochs, Learning Rate: 0.05\n",
      "=============================================================\n",
      "Epoch 1: training loss 0.388, valid. loss 1.891, valid. accuracy 0.520, and valid. RMSE 0.980\n",
      "Epoch 11: training loss 0.260, valid. loss 2.371, valid. accuracy 0.493, and valid. RMSE 1.013\n",
      "Epoch 21: training loss 0.284, valid. loss 2.202, valid. accuracy 0.547, and valid. RMSE 1.026\n",
      "Epoch 31: training loss 0.259, valid. loss 2.097, valid. accuracy 0.547, and valid. RMSE 1.026\n",
      "Epoch 41: training loss 0.208, valid. loss 2.454, valid. accuracy 0.467, and valid. RMSE 1.102\n",
      "Epoch 51: training loss 0.243, valid. loss 2.724, valid. accuracy 0.507, and valid. RMSE 1.007\n",
      "Epoch 61: training loss 0.163, valid. loss 2.583, valid. accuracy 0.467, and valid. RMSE 1.007\n",
      "Epoch 71: training loss 0.220, valid. loss 2.598, valid. accuracy 0.493, and valid. RMSE 1.052\n",
      "Epoch 81: training loss 0.161, valid. loss 2.464, valid. accuracy 0.547, and valid. RMSE 1.007\n",
      "Epoch 91: training loss 0.168, valid. loss 2.665, valid. accuracy 0.547, and valid. RMSE 0.966\n",
      "FINAL: training loss 0.172, valid. loss 2.789, valid. accuracy 0.507, and valid. RMSE 1.007\n",
      "\n",
      "BiLSTM - Variable Length: 100 epochs, Learning Rate: 0.01\n",
      "=============================================================\n",
      "Epoch 1: training loss 0.192, valid. loss 2.752, valid. accuracy 0.547, and valid. RMSE 0.966\n",
      "Epoch 11: training loss 0.209, valid. loss 2.723, valid. accuracy 0.520, and valid. RMSE 1.000\n",
      "Epoch 21: training loss 0.133, valid. loss 2.720, valid. accuracy 0.493, and valid. RMSE 1.013\n",
      "Epoch 31: training loss 0.198, valid. loss 2.848, valid. accuracy 0.493, and valid. RMSE 1.013\n",
      "Epoch 41: training loss 0.208, valid. loss 2.897, valid. accuracy 0.520, and valid. RMSE 1.000\n",
      "Epoch 51: training loss 0.126, valid. loss 2.730, valid. accuracy 0.533, and valid. RMSE 0.993\n",
      "Epoch 61: training loss 0.149, valid. loss 2.751, valid. accuracy 0.520, and valid. RMSE 1.020\n",
      "Epoch 71: training loss 0.177, valid. loss 2.818, valid. accuracy 0.520, and valid. RMSE 1.000\n",
      "Epoch 81: training loss 0.111, valid. loss 2.821, valid. accuracy 0.507, and valid. RMSE 1.007\n",
      "Epoch 91: training loss 0.190, valid. loss 2.836, valid. accuracy 0.507, and valid. RMSE 1.026\n",
      "FINAL: training loss 0.137, valid. loss 2.813, valid. accuracy 0.493, and valid. RMSE 1.033\n"
     ]
    }
   ],
   "source": [
    "model = BiLstmVariableLength(vocab_size, EMBEDDED_LAYER_DIM, HIDDEN_LAYER_DIM)\n",
    "\n",
    "print(f'\\nBiLSTM - Variable Length: {EPOCHS} epochs, Learning Rate: 0.1')\n",
    "print('=============================================================')\n",
    "train_model(model, epochs=EPOCHS, lr=0.1)\n",
    "print(f'\\nBiLSTM - Variable Length: {EPOCHS} epochs, Learning Rate: 0.05')\n",
    "print('=============================================================')\n",
    "train_model(model, epochs=EPOCHS, lr=0.05)\n",
    "print(f'\\nBiLSTM - Variable Length: {EPOCHS} epochs, Learning Rate: 0.01')\n",
    "print('=============================================================')\n",
    "train_model(model, epochs=EPOCHS, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## BiLSTM - with pretrained GloVe Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BiLSTM - with pretrained GloVe Word Embeddings: 100 epochs, Learning Rate: 0.1\n",
      "====================================================================================\n",
      "Epoch 1: training loss 1.438, valid. loss 1.236, valid. accuracy 0.320, and valid. RMSE 0.825\n",
      "Epoch 11: training loss 0.930, valid. loss 1.024, valid. accuracy 0.427, and valid. RMSE 1.026\n",
      "Epoch 21: training loss 0.701, valid. loss 1.352, valid. accuracy 0.440, and valid. RMSE 1.020\n",
      "Epoch 31: training loss 0.671, valid. loss 1.310, valid. accuracy 0.480, and valid. RMSE 1.020\n",
      "Epoch 41: training loss 0.609, valid. loss 1.275, valid. accuracy 0.427, and valid. RMSE 1.102\n",
      "Epoch 51: training loss 0.534, valid. loss 1.475, valid. accuracy 0.480, and valid. RMSE 0.980\n",
      "Epoch 61: training loss 0.492, valid. loss 1.532, valid. accuracy 0.493, and valid. RMSE 0.993\n",
      "Epoch 71: training loss 0.412, valid. loss 1.508, valid. accuracy 0.467, and valid. RMSE 1.102\n",
      "Epoch 81: training loss 0.465, valid. loss 1.669, valid. accuracy 0.480, and valid. RMSE 0.980\n",
      "Epoch 91: training loss 0.372, valid. loss 1.899, valid. accuracy 0.480, and valid. RMSE 1.039\n",
      "FINAL: training loss 0.328, valid. loss 1.798, valid. accuracy 0.493, and valid. RMSE 1.013\n",
      "\n",
      "BiLSTM - with pretrained GloVe Word Embeddings: 100 epochs, Learning Rate: 0.05\n",
      "====================================================================================\n",
      "Epoch 1: training loss 0.387, valid. loss 1.729, valid. accuracy 0.507, and valid. RMSE 0.987\n",
      "Epoch 11: training loss 0.333, valid. loss 1.854, valid. accuracy 0.560, and valid. RMSE 0.959\n",
      "Epoch 21: training loss 0.305, valid. loss 1.944, valid. accuracy 0.507, and valid. RMSE 0.987\n",
      "Epoch 31: training loss 0.277, valid. loss 2.316, valid. accuracy 0.480, and valid. RMSE 0.980\n",
      "Epoch 41: training loss 0.287, valid. loss 2.284, valid. accuracy 0.467, and valid. RMSE 1.083\n",
      "Epoch 51: training loss 0.271, valid. loss 2.261, valid. accuracy 0.427, and valid. RMSE 1.102\n",
      "Epoch 61: training loss 0.257, valid. loss 2.489, valid. accuracy 0.440, and valid. RMSE 1.114\n",
      "Epoch 71: training loss 0.276, valid. loss 2.590, valid. accuracy 0.467, and valid. RMSE 1.026\n",
      "Epoch 81: training loss 0.263, valid. loss 2.500, valid. accuracy 0.467, and valid. RMSE 1.007\n",
      "Epoch 91: training loss 0.281, valid. loss 2.727, valid. accuracy 0.440, and valid. RMSE 1.077\n",
      "FINAL: training loss 0.240, valid. loss 2.566, valid. accuracy 0.440, and valid. RMSE 1.131\n",
      "\n",
      "BiLSTM - with pretrained GloVe Word Embeddings: 100 epochs, Learning Rate: 0.01\n",
      "====================================================================================\n",
      "Epoch 1: training loss 0.231, valid. loss 2.612, valid. accuracy 0.493, and valid. RMSE 1.052\n",
      "Epoch 11: training loss 0.258, valid. loss 2.644, valid. accuracy 0.440, and valid. RMSE 1.149\n",
      "Epoch 21: training loss 0.204, valid. loss 2.618, valid. accuracy 0.480, and valid. RMSE 1.114\n",
      "Epoch 31: training loss 0.198, valid. loss 2.671, valid. accuracy 0.493, and valid. RMSE 1.089\n",
      "Epoch 41: training loss 0.219, valid. loss 2.763, valid. accuracy 0.453, and valid. RMSE 1.125\n",
      "Epoch 51: training loss 0.201, valid. loss 2.857, valid. accuracy 0.467, and valid. RMSE 1.120\n",
      "Epoch 61: training loss 0.217, valid. loss 2.858, valid. accuracy 0.453, and valid. RMSE 1.125\n",
      "Epoch 71: training loss 0.184, valid. loss 2.978, valid. accuracy 0.427, and valid. RMSE 1.137\n",
      "Epoch 81: training loss 0.193, valid. loss 2.956, valid. accuracy 0.440, and valid. RMSE 1.149\n",
      "Epoch 91: training loss 0.176, valid. loss 3.124, valid. accuracy 0.413, and valid. RMSE 1.160\n",
      "FINAL: training loss 0.187, valid. loss 2.980, valid. accuracy 0.453, and valid. RMSE 1.108\n"
     ]
    }
   ],
   "source": [
    "def load_glove_vectors():\n",
    "    \"\"\"Load the glove Global Vectors for Word Representation\"\"\"\n",
    "    word_vectors = {}\n",
    "\n",
    "    with open(\"./data/glove/glove.6B.50d.txt\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            split = line.split()\n",
    "            word_vectors[split[0]] = np.array([float(x) for x in split[1:]])\n",
    "    return word_vectors\n",
    "\n",
    "\n",
    "def get_embedding_matrix(word_counts, emb_size=50):\n",
    "    \"\"\" Creates embedding matrix from word vectors\"\"\"\n",
    "    vocab_size = len(word_counts) + 2\n",
    "    vocab_to_idx = {}\n",
    "    vocab = [\"\", \"UNK\"]\n",
    "    W = np.zeros((vocab_size, emb_size), dtype=\"float32\")\n",
    "    W[0] = np.zeros(emb_size, dtype='float32')  # adding a vector for padding\n",
    "    W[1] = np.random.uniform(-0.25, 0.25, emb_size)  # adding a vector for unknown words\n",
    "    vocab_to_idx[\"UNK\"] = 1\n",
    "    i = 2\n",
    "\n",
    "    for word in word_counts:\n",
    "        if word in word_vecs:\n",
    "            W[i] = word_vecs[word]\n",
    "        else:\n",
    "            W[i] = np.random.uniform(-0.25, 0.25, emb_size)\n",
    "        vocab_to_idx[word] = i\n",
    "        vocab.append(word)\n",
    "        i += 1\n",
    "    return W, np.array(vocab), vocab_to_idx\n",
    "\n",
    "\n",
    "word_vecs = load_glove_vectors()\n",
    "pretrained_weights, vocab, vocab2index = get_embedding_matrix(counts, EMBEDDED_LAYER_DIM)\n",
    "\n",
    "\n",
    "model = BiLstmGloveVector(vocab_size, EMBEDDED_LAYER_DIM, HIDDEN_LAYER_DIM, pretrained_weights)\n",
    "\n",
    "print(f'\\nBiLSTM - with pretrained GloVe Word Embeddings: {EPOCHS} epochs, Learning Rate: 0.1')\n",
    "print('====================================================================================')\n",
    "train_model(model, epochs=EPOCHS, lr=0.1)\n",
    "print(f'\\nBiLSTM - with pretrained GloVe Word Embeddings: {EPOCHS} epochs, Learning Rate: 0.05')\n",
    "print('====================================================================================')\n",
    "train_model(model, epochs=EPOCHS, lr=0.05)\n",
    "print(f'\\nBiLSTM - with pretrained GloVe Word Embeddings: {EPOCHS} epochs, Learning Rate: 0.01')\n",
    "print('====================================================================================')\n",
    "train_model(model, epochs=EPOCHS, lr=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (AolmeScripts)",
   "language": "python",
   "name": "pycharm-d092e169"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}